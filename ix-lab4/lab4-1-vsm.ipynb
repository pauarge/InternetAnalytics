{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *O*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Argelaguet Franquelo, Pau*\n",
    "* *du Bois de Dunilac, Vivien*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import operator\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def filter_word(word):\n",
    "    # Removing words in stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    # Removing words consisting of a punctuation sign\n",
    "    if word in string.punctuation:\n",
    "        return False\n",
    "    # Removing numbers\n",
    "    if is_number(word):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_word(word):\n",
    "    # Removing punctuation signs from word\n",
    "    word = \"\".join(c for c in word if c not in string.punctuation)\n",
    "    # Transforming word to lowercase\n",
    "    word = word.lower()\n",
    "    # Stemming\n",
    "    word = stemmer.stem(word)\n",
    "    return word\n",
    "    \n",
    "\n",
    "def get_bag_of_words(text):\n",
    "    words = filter(filter_word, map(clean_word, text.split()))\n",
    "    bow = {}\n",
    "    for w in words:\n",
    "        if w in bow:\n",
    "            bow[w] += 1\n",
    "        else:\n",
    "            bow[w] = 1\n",
    "    \n",
    "    # Removing less frequent words\n",
    "    bow = {k: v for k, v in bow.items() if v > 1}\n",
    "            \n",
    "    return dict(collections.OrderedDict(sorted(bow.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = {\n",
    "    x.get('courseId'): {\n",
    "        'name': x.get('name'),\n",
    "        'description': get_bag_of_words(x.get('description'))\n",
    "    } for x in courses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_terms = list(x.get('description') for x in dat.values())\n",
    "\n",
    "terms = {}\n",
    "for l in list_terms:\n",
    "    for k, v in l.items():\n",
    "        if k in terms:\n",
    "            terms[k] += v\n",
    "        else:\n",
    "            terms[k] = v\n",
    "            \n",
    "terms = dict(collections.OrderedDict(sorted(terms.items(), key=operator.itemgetter(1), reverse=True)))\n",
    "top_terms = list({k: v for k, v in terms.items() if v > 400}.keys())\n",
    "low_terms = list({k: v for k, v in terms.items() if v < 3}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dat.items():\n",
    "    dat[k]['description'] = {x: y for x, y in v.get('description').items() if x not in top_terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad': 2,\n",
       " 'algebra': 2,\n",
       " 'algorithm': 2,\n",
       " 'analyt': 2,\n",
       " 'applic': 2,\n",
       " 'auction': 2,\n",
       " 'base': 2,\n",
       " 'class': 3,\n",
       " 'cluster': 2,\n",
       " 'communiti': 2,\n",
       " 'comput': 2,\n",
       " 'data': 6,\n",
       " 'dataset': 2,\n",
       " 'detect': 2,\n",
       " 'ecommerc': 2,\n",
       " 'explor': 5,\n",
       " 'graph': 2,\n",
       " 'hadoop': 2,\n",
       " 'homework': 2,\n",
       " 'inform': 2,\n",
       " 'internet': 2,\n",
       " 'lab': 3,\n",
       " 'largescal': 3,\n",
       " 'linear': 2,\n",
       " 'machin': 2,\n",
       " 'mine': 3,\n",
       " 'network': 4,\n",
       " 'number': 2,\n",
       " 'onlin': 5,\n",
       " 'practic': 2,\n",
       " 'problem': 2,\n",
       " 'realworld': 4,\n",
       " 'recommend': 3,\n",
       " 'relat': 2,\n",
       " 'servic': 3,\n",
       " 'session': 2,\n",
       " 'social': 5,\n",
       " 'stream': 2}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['COM-308']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocess.pckl\", \"wb\") as f:\n",
    "    pickle.dump(dat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_terms = [list(x.get('description').keys()) for x in dat.values()]\n",
    "\n",
    "terms = set()\n",
    "for l in list_terms:\n",
    "    terms = terms.union(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(dat.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
