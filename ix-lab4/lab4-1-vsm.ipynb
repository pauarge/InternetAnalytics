{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *O*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Argelaguet Franquelo, Pau*\n",
    "* *du Bois de Dunilac, Vivien*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import operator\n",
    "import math\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl, save_pkl\n",
    "\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer for posterior use\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if given word is a number\n",
    "def is_number(word):\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    \n",
    "# If the word passes the filters and should be in the dataset, returns True, False otherwise\n",
    "def filter_word(word):\n",
    "    # Words of len 1\n",
    "    if len(word) < 2:\n",
    "        return False\n",
    "    # Removing words in stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    # Removing words consisting of a punctuation sign\n",
    "    if word in string.punctuation:\n",
    "        return False\n",
    "    # Removing numbers\n",
    "    if is_number(word):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Changes the word in a common format easier to group\n",
    "def clean_word(word):\n",
    "    # Removing punctuation signs from word\n",
    "    word = \"\".join(c for c in word if c not in string.punctuation)\n",
    "    # Transforming word to lowercase\n",
    "    word = word.lower()\n",
    "    # Stemming\n",
    "    word = stemmer.stem(word)\n",
    "    return word\n",
    "    \n",
    "\n",
    "def get_bag_of_words(text):\n",
    "    # Splitting the text in whitespaces, cleaning each word and the filtering\n",
    "    words = filter(filter_word, map(clean_word, text.split()))\n",
    "    \n",
    "    # Creating bag-of-words counters\n",
    "    bow = collections.defaultdict(int)\n",
    "    for w in words:\n",
    "        bow[w] += 1\n",
    "    \n",
    "    # Removing less frequent words\n",
    "    bow = {k: v for k, v in bow.items() if v > 1}\n",
    "            \n",
    "    return dict(collections.OrderedDict(sorted(bow.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary of dictionaries of the form \n",
    "# {'courseId' -> {name, description}}\n",
    "dat = {\n",
    "    x.get('courseId'): {\n",
    "        'name': x.get('name'),\n",
    "        'description': get_bag_of_words(x.get('description'))\n",
    "    } for x in courses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all bags of words\n",
    "list_terms = [x.get('description') for x in dat.values()]\n",
    "\n",
    "# Dict of all terms and their frequency\n",
    "term_freqs = collections.defaultdict(int)\n",
    "for l in list_terms:\n",
    "    for k, v in l.items():\n",
    "        term_freqs[k] += v\n",
    "term_freqs = dict(collections.OrderedDict(\n",
    "    sorted(term_freqs.items(), key=operator.itemgetter(1), reverse=True)))\n",
    "\n",
    "# List of terms which appear more than 400 times\n",
    "top_terms = list({k: v for k, v in term_freqs.items() if v > 400}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing documents that are in top_terms\n",
    "for k, v in dat.items():\n",
    "    dat[k]['description'] = {x: y for x, y in v.get('description').items() \n",
    "                             if x not in top_terms}\n",
    "    \n",
    "# Ensuring all documents have description\n",
    "dat = {k: v for k, v in dat.items() if len(v.get('description')) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing task, the following tasks have been implemented: \n",
    "\n",
    "* Removal of the stopwords by using the provided list, because they do not provide information on the document.\n",
    "* Removal of the punctuation signs, both standalone signs and signs inside words because they carry no meaning.\n",
    "* Removal of very frequent words because they appear often in all documents and therefore we are not able to differenciate one document from another using those.\n",
    "* Removal of numbers because they again not carry special meaning by themselves.\n",
    "* Removal of short words (length == 1).\n",
    "* Lowercase and stem the words so similar words can be grouped and counted as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet Analytics terms after preprocess:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ad': 2,\n",
       " 'algebra': 2,\n",
       " 'algorithm': 2,\n",
       " 'analyt': 2,\n",
       " 'applic': 2,\n",
       " 'auction': 2,\n",
       " 'base': 2,\n",
       " 'class': 3,\n",
       " 'cluster': 2,\n",
       " 'communiti': 2,\n",
       " 'comput': 2,\n",
       " 'data': 6,\n",
       " 'dataset': 2,\n",
       " 'detect': 2,\n",
       " 'ecommerc': 2,\n",
       " 'explor': 5,\n",
       " 'graph': 2,\n",
       " 'hadoop': 2,\n",
       " 'homework': 2,\n",
       " 'inform': 2,\n",
       " 'internet': 2,\n",
       " 'lab': 3,\n",
       " 'largescal': 3,\n",
       " 'linear': 2,\n",
       " 'machin': 2,\n",
       " 'mine': 3,\n",
       " 'network': 4,\n",
       " 'number': 2,\n",
       " 'onlin': 5,\n",
       " 'practic': 2,\n",
       " 'problem': 2,\n",
       " 'realworld': 4,\n",
       " 'recommend': 3,\n",
       " 'relat': 2,\n",
       " 'servic': 3,\n",
       " 'session': 2,\n",
       " 'social': 5,\n",
       " 'stream': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Internet Analytics terms after preprocess:\")\n",
    "dat['COM-308']['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sorted list of all terms in the dataset.\n",
    "terms = set()\n",
    "for l in [list(x.get('description').keys()) for x in dat.values()]:\n",
    "    terms = terms.union(set(l))\n",
    "terms = sorted(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sorted list of all documents in the dataset\n",
    "documents = sorted(list(dat.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(terms)\n",
    "N = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from term to index in terms list\n",
    "term_idx = {x: terms.index(x) for x in terms}\n",
    "\n",
    "# Mapping from document to index in documents list\n",
    "doc_idx = {x: documents.index(x) for x in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating IDF for all terms in dataset\n",
    "occs = collections.defaultdict(float)\n",
    "for k, v in dat.items():\n",
    "    for x in v.get('description').keys():\n",
    "        occs[x] += 1\n",
    "idf = {k: math.log(N/v) for k, v in occs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrix with TF-IDF values\n",
    "values = []\n",
    "rows = []\n",
    "columns = []\n",
    "\n",
    "for k, v in dat.items():\n",
    "    d = v.get('description')\n",
    "    max_occur = float(max(d.values()))\n",
    "    for x, y in d.items():\n",
    "        tf = y / max_occur\n",
    "        tfidf = tf * idf[x]\n",
    "        values.append(tfidf)\n",
    "        rows.append(term_idx[x])\n",
    "        columns.append(doc_idx[k])\n",
    "        \n",
    "mat = csr_matrix((values, (rows, columns)), shape=(M, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 terms in the description of IX with highest TF-IDF:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('onlin', 3.9895764523183717),\n",
       " ('explor', 3.473710445313186),\n",
       " ('social', 3.473710445313186),\n",
       " ('realworld', 3.415975986268839),\n",
       " ('mine', 2.6735537653587342),\n",
       " ('largescal', 2.3269801750787615),\n",
       " ('auction', 2.2444672972791198),\n",
       " ('ecommerc', 2.2444672972791198),\n",
       " ('hadoop', 2.2444672972791198),\n",
       " ('data', 2.222542385320509),\n",
       " ('servic', 2.2154083994216567),\n",
       " ('network', 1.9661415052793987),\n",
       " ('ad', 1.8782632010564164),\n",
       " ('internet', 1.8782632010564164),\n",
       " ('communiti', 1.7823691769058227)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = mat.getcol(doc_idx['COM-308'])\n",
    "ts = {terms[x]: v for (x, _), v in c.todok().items()}\n",
    "ts = sorted(ts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"15 terms in the description of IX with highest TF-IDF:\")\n",
    "ts[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large scores in TF-IDF mean that the specific term is very representative of the document, that is, it's common in the document itself but not in the other documents of the corpus. In this case, *onlin* is really related with IX and we could characterize the course using that term, while *communiti* is as well, but it might not be sufficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comupte similarity between two vectors using the provided formula\n",
    "def compute_sim(di, dj):\n",
    "    res = (di.T).dot(dj) / (np.linalg.norm(di) * np.linalg.norm(dj))\n",
    "    return float(res)\n",
    "\n",
    "\n",
    "# Get document similar to a text \n",
    "def search(t):\n",
    "    # Splitting and stemming input text into words\n",
    "    search_terms = list(map(stemmer.stem, t.split()))\n",
    "    \n",
    "    # Creating vector representing query\n",
    "    v = [idf[x] for x in search_terms]\n",
    "    r = [term_idx[x] for x in search_terms]\n",
    "    c = [0 for i in search_terms]\n",
    "    q = csr_matrix((v, (r, c)), shape=(M, 1)).todense()\n",
    "    \n",
    "    # Computing similarities of all columns (documents) with given query\n",
    "    sims = [compute_sim(q, mat.getcol(i).todense()) for i in range(N)]\n",
    "    sims = {dat[documents[i]]['name']: x for i, x in enumerate(sims) if x > 0}  \n",
    "    sims = sorted(sims.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for markov chains:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Applied probability & stochastic processes', 0.6662086921572952),\n",
       " ('Applied stochastic processes', 0.6348838757503898),\n",
       " ('Markov chains and algorithmic applications', 0.4639051570628536),\n",
       " ('Supply chain management', 0.378044375439343),\n",
       " ('Statistical Sequence Processing', 0.36051211888098916)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 courses for markov chains:\")\n",
    "search('markov chains')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for facebook:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computational Social Media', 0.14297951505673218)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 courses for facebook:\")\n",
    "search('facebook')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For markov chains, we get five courses that judging by the title, they are related to the concept of a markov chain. That is because in the description they have explicitly the words *markov* and *chain*. The problem with *facebook* is that we only get one course, which is the only one in the dataset that contains explicitly the workd *facebook*. We do not get anitying else even though they might talk about the concept of social media, because in this current model we are not taking into account concepts, just explicit terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data for other exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(dat, \"data/preprocess.pckl\")\n",
    "save_pkl(mat, \"data/mat.pckl\")\n",
    "save_pkl(terms, \"data/terms.pckl\")\n",
    "save_pkl(documents, \"data/documents.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
